# Теория по алгоритмам

## Вопросы

"Дайте оценку сложности алгоритма",
"расскажите о вариантах обхода дерева",
“напишите свою реализацию интерфейса LinkedList”

Базовые:

- линейный поиск
- двоичный поиск
- пузырьковая сортировка
- быстрая сортировка

## Оценка сложности алгоритмов Big O

Сложность алгоритмов оценивают по времени выполнения задачи или по используемой
памяти.
Сложность зависит от размеров входных данных: массив из 100 элементов будет
обработан быстрее, чем аналогичный из 1000.

Big O показывает верхнюю границу (худший вариант) того, как сложность алгоритма
растёт с увеличением входных данных.

**Сложность алгоритма может быть:**

**О(1)** – константная сложность. Время выполнения алгоритма не зависит от
размера входных данных. Например: получить элемент массива по индексу.

**O(log n)** – логарифмическая сложность<br>
Сложность алгоритма растёт логарифмически с увеличением входных данных.
Например, бинарный поиск в отсортированном массиве.

**О(n)** – линейная сложность. Сложность алгоритма растёт линейно с увеличением
входных данных. Например: цикл по всем элементам массива, рекурсивная функция.

**O(n log n)** - Линейно-логарифмическая сложность. Время выполнения алгоритма
растет быстрее, чем линейно, но медленнее, чем квадратично. Например, сортировка
слиянием (merge sort).

**O(n^2)** - Квадратичная сложность. Время выполнения алгоритма зависит от
квадрата размера входных данных. Например, сортировка пузырьком (bubble sort).

**O(n^3)** - Кубическая сложность. Время выполнения алгоритма зависит от размера
входных данных в кубе. Например, алгоритмы, которые имеют три вложенных цикла,
такие как некоторые методы многомерной обработки данных.

**O(n!)** - Факториальная сложность. Это самая высокая степень роста времени
выполнения алгоритма. Время выполнения алгоритма растет факториально от размера
входных данных. Этот тип сложности встречается, например, при переборе всех
возможных комбинаций элементов, что делает его чрезвычайно неэффективным для
больших значений n.

**Другие обозначения сложности алгоритмов.**<br>
Кроме обозначения "Big O", существуют другие обозначения для оценки сложности
алгоритмов.

- Big Theta (Θ): Big Theta также оценивает верхнюю и нижнюю границы временной
  сложности алгоритма, но описывает точную сложность, а не только наихудший
  случай, как Big O. Θ(f(n)) обозначает, что время выполнения алгоритма
  ограничено функцией f(n) как сверху, так и снизу.
- Big Omega (Ω): Big Omega оценивает нижнюю границу временной сложности
  алгоритма. Ω(f(n)) говорит о том, что алгоритм выполнится не быстрее, чем
  функция f(n).
- Little O (o): Little O представляет собой верхнюю границу, которая строже, чем
  Big O. Если f(n) является o(g(n)), это означает, что время выполнения
  алгоритма ограничивается функцией g(n), но алгоритм работает быстрее, чем g(
  n).
- Little Omega (ω): Little Omega представляет собой нижнюю границу, которая
  строже, чем Big Omega. Если f(n) является ω(g(n)), это означает, что алгоритм
  работает медленнее, чем g(n), но не медленнее, чем f(n).

### Константная сложность O(1)

O(1) называют константной сложностью.

Оценка временной сложности O(1) означает, что алгоритм имеет постоянную
сложность.

Под n мы будем иметь ввиду размер входящих данных.

В JavaScript чаще всего мы храним набор данных в массивах.

Размер массива, то есть его значение свойства length и будет значение n.

При константной сложности вне зависимости от размера входных данных (n), время
выполнения алгоритма остается постоянным и не зависит от объема данных.

Это самый быстрый и эффективный вид временной сложности.

Примеры алгоритмов с оценкой временной сложности O(1):

- Доступ к элементу в массиве по индексу. Например, если у вас есть массив с
  данными, вы можете мгновенно получить доступ к элементу массива, указав его
  индекс, независимо от размера массива.

- Вставка или удаление элемента в конец списка (очереди) фиксированной длины. В
  этом случае операция выполняется быстро и не зависит от количества элементов в
  списке.

Оценка временной сложности O(1) является идеальной с точки зрения
производительности. Однако в реальных задачах она не всегда достижима, и в
большинстве случаев оценка временной сложности будет выше.

```javascript
    function getLastElement(arr) {
    return arr[arr.length - 1]
}

```

Функция getLastElement имеет временную сложность O(1) потому, что она выполняет
одну операцию - получение последнего элемента массива arr.

Независимо от размера массива, функция всегда выполняет одно действие, поэтому
ее сложность остается постоянной и равной O(1). Это означает, что время
выполнения функции не зависит от количества элементов в массиве и всегда будет
быстрым и постоянным.

### Линейная сложность O(n)

O(n) называют линейной сложностью.

Линейный рост - это понятие, описывающее зависимость между двумя
величинами, при которой одна величина увеличивается пропорционально увеличению
другой.

Оценка временной сложности O(n) означает, что время выполнения алгоритма растет
линейно с увеличением размера входных данных.

```javascript
    function findMax(arr) {
    let max = arr[0];
    for (let i = 1; i < arr.length; i++) {
        if (arr[i] > max) {
            max = arr[i];
        }
    }
    return max;
}

```

В этом примере findMax ищет максимальное значение в массиве arr.

Алгоритм начинает с предположения, что первый элемент массива (arr[0]) является
максимальным, а затем линейно (то есть по одному элементу) перебирает остальные
элементы, сравнивая каждый с текущим максимальным. Сложность этого алгоритма O(
n), так как время выполнения растет линейно с увеличением количества элементов в
массиве arr.

При линейном росте при увеличении размера входных данных вдвое, то время
выполнения алгоритма также увеличится примерно вдвое. Если увеличите размер
данных в 10 раз, то время выполнения увеличится приблизительно в 10 раз, и так
далее.

Линейный рост характерен для алгоритмов, которые выполняют постоянное количество
операций для каждого элемента входных данных.

--------------------------------------------------------------------------------

### Логарифмическая сложность O(log n)

Логарифмическая сложность алгоритма — это характеристика времени выполнения или
потребления ресурсов, при которой время работы алгоритма растет пропорционально
логарифму от размера входных данных. Обозначается как **O(log n)**, где **n** —
размер входных данных.

### Объяснение

- Логарифм (обычно с основанием 2 в контексте алгоритмов) отражает, сколько раз
  можно разделить входные данные на части (например, пополам), чтобы обработать
  их.
- Алгоритмы с логарифмической сложностью очень эффективны, особенно для больших
  объемов данных, так как время выполнения растет медленно даже при значительном
  увеличении **n**.

### Примеры алгоритмов с логарифмической сложностью

1. **Бинарный поиск**:

- Делит отсортированный массив пополам на каждом шаге, чтобы найти элемент.
- Сложность: **O(log n)**.
- Пример: Поиск числа в массиве из 1024 элементов требует не более 10 итераций (
  log₂(1024) = 10).

2. **Операции в сбалансированных деревьях**:

- Вставка, удаление или поиск в структурах, таких как AVL-дерево или
  красно-черное дерево, имеют сложность **O(log n)**, так как высота дерева
  пропорциональна логарифму числа узлов.

3. **Алгоритмы типа "разделяй и властвуй"** (частично):

- Некоторые этапы, например разбиение в бинарной куче или в алгоритмах, подобных
  быстрой сортировке (в среднем), имеют логарифмическую составляющую.

### Почему логарифмическая сложность эффективна?

- Если **n = 1,000,000**, то **log₂(n) ≈ 20**. Это значит, что алгоритм выполнит
  всего около 20 шагов, что очень быстро даже для больших данных.
- Сравните с линейной сложностью **O(n)** (1,000,000 шагов) или квадратичной *
  *O(n²)** (1,000,000,000,000 шагов).

### Визуализация

| Размер входных данных (n) | O(log n) (итераций) |
|---------------------------|---------------------|
| 10                        | ~3                  |
| 100                       | ~7                  |
| 1,000                     | ~10                 |
| 1,000,000                 | ~20                 |

### Когда встречается?

Логарифмическая сложность типична для алгоритмов, которые:

- Делят задачу на меньшие части (обычно пополам) на каждом шаге.
- Используют структуры данных с логарифмической высотой (например, деревья).
- Решают задачи с помощью рекурсии, где размер задачи быстро уменьшается.

### Пример в Java (бинарный поиск)

```java
public int binarySearch(int[] arr, int target) {
    int left = 0, right = arr.length - 1;
    while (left <= right) {
        int mid = left + (right - left) / 2;
        if (arr[mid] == target) return mid;
        if (arr[mid] < target) left = mid + 1;
        else right = mid - 1;
    }
    return -1;
}
```

- Сложность: **O(log n)**, так как массив делится пополам на каждом шаге.

### Итог

Логарифмическая сложность (**O(log n)
**) означает, что алгоритм очень масштабируем и эффективен для больших данных,
так как его производительность почти не ухудшается с ростом
**n
**. Если у вас есть конкретный алгоритм или контекст, могу разобрать подробнее!

======== GROK: O(log(n))

**Сложность алгоритма (O(log n))**, или **логарифмическая сложность**,
означает, что время выполнения алгоритма (или количество операций) растёт
пропорционально логарифму от размера входных данных (n). Это одна из самых
эффективных сложностей для больших данных, так как рост времени замедляется по
мере увеличения (n).

### Объяснение:

- Логарифмическая сложность обычно возникает в алгоритмах, которые на каждой
  итерации **делят задачу на части** (например, на половину), значительно
  сокращая объем работы.
- Логарифм (обычно основание 2, \( \log_2 n \)) показывает, сколько раз можно
  разделить \( n \) на 2, чтобы получить 1.
    - Например, если \( n = 8 \), то \( \log_2 8 = 3 \), так как \( 8 \div 2 =
      4 \), \( 4 \div 2 = 2 \), \( 2 \div 2 = 1 \) (3 шага).
    - Для \( n = 1024 \), \( \log_2 1024 = 10 \).

### Примеры алгоритмов с \( O(\log n) \):

1. **Бинарный поиск (Binary Search)**:

- На каждой итерации массив делится пополам, пока не найдется элемент.
- Пример: Поиск в отсортированном массиве.
- Сложность: \( O(\log n) \).

2. **Операции в сбалансированных бинарных деревьях поиска (BST)**:

- Вставка, поиск и удаление в AVL-дереве или красно-черном дереве (например,
  в `TreeMap` или `TreeSet` в Java).
- Высота дерева \( \approx \log n \), поэтому операции занимают \( O(\log
  n) \).

3. **Двоичный поиск в структурах данных**:

- Например, поиск в отсортированном наборе или определение границ в задачах.

### Почему \( O(\log n) \) эффективно?

- Для больших \( n \) логарифмическая сложность растёт очень медленно:
    - \( n = 10^3 \): \( \log_2 n \approx 10 \).
    - \( n = 10^6 \): \( \log_2 n \approx 20 \).
    - \( n = 10^9 \): \( \log_2 n \approx 30 \).
- Это делает алгоритмы с \( O(\log n) \) подходящими для обработки огромных
  объемов данных.

### Сравнение с другими сложностями:

- \( O(1) \): Постоянное время, не зависит от \( n \).
- \( O(\log n) \): Логарифмическое время, очень эффективно.
- \( O(n) \): Линейное время, пропорционально \( n \).
- \( O(n \log n) \): Линейно-логарифмическое, типично для сортировок (например,
  `Arrays.sort` в Java).
- \( O(n^2) \): Квадратичное время, менее эффективно для больших \( n \).

### Пример в Java (бинарный поиск):

```java
public int binarySearch(int[] arr, int target) {
    int left = 0;
    int right = arr.length - 1;

    while (left <= right) {
        int mid = left + (right - left) / 2; // Находим середину
        if (arr[mid] == target) {
            return mid;
        } else if (arr[mid] < target) {
            left = mid + 1; // Исключаем левую половину
        } else {
            right = mid - 1; // Исключаем правую половину
        }
    }
    return -1; // Элемент не найден
}
```

- На каждой итерации размер рассматриваемого массива уменьшается вдвое, поэтому
  сложность \( O(\log n) \).

### Связь с бинарным деревом поиска:

В сбалансированном бинарном дереве поиска (BST) высота дерева составляет \( O(
\log n) \), так как каждый уровень делит узлы примерно пополам. Это обеспечивает
логарифмическую сложность для операций поиска, вставки и удаления. Однако в
несбалансированном BST (например, вырожденном в список) сложность может
ухудшиться до \( O(n) \).

### Заключение:

\( O(\log n) \) описывает алгоритмы, которые эффективно сокращают объем работы
на каждой итерации, разделяя данные на части (обычно пополам). Это характерно
для бинарного поиска, операций в сбалансированных деревьях и других алгоритмов,
работающих с иерархическими структурами. Если нужен пример конкретного алгоритма
или дополнительное объяснение, дайте знать!

--- 

### Квадратичная сложность O(n^2)

O(n^2) означает квадратичную сложность алгоритма, где время выполнения растет
пропорционально квадрату размера входных данных. Это часто возникает в
алгоритмах с вложенными циклами, когда каждый элемент первого списка
обрабатывается с каждым элементом второго списка.

Вот пример простого алгоритма с квадратичной сложностью, который ищет сумму всех
пар элементов в массиве:

```javascript
function sumOfPairs(arr) {
    let sum = 0;
    for (let i = 0; i < arr.length; i++) {
        for (let j = 0; j < arr.length; j++) {
            sum += arr[i] + arr[j];
        }
    }
    return sum;
}

const myArray = [1, 2, 3, 4];
console.log(sumOfPairs(myArray)); // Результат: 40
```

Этот код имеет два вложенных цикла, каждый из которых проходится по всему
массиву. Количество операций в циклах равно n * n, где n - это длина массива.
Это приводит к квадратичной сложности O(n^2), что может сделать его
неэффективным для больших массивов из-за большого количества операций,
выполняемых на каждый элемент.

### Кубическая сложность O(n^3)

Cложность O(n^3) означает, что время выполнения алгоритма увеличивается
кубически по размеру входных данных. Это часто встречается в алгоритмах, где
есть три вложенных цикла или операции, каждая из которых выполняется
пропорционально кубу размера входных данных.

Вот пример алгоритма с кубической сложностью, который умножает две матрицы:

```javascript
function multiplyMatrices(matrix1, matrix2) {
    let result = [];
    const m = matrix1.length;
    const n = matrix2[0].length;
    const p = matrix2.length;

    for (let i = 0; i < m; i++) {
        result[i] = [];
        for (let j = 0; j < n; j++) {
            result[i][j] = 0;
            for (let k = 0; k < p; k++) {
                result[i][j] += matrix1[i][k] * matrix2[k][j];
            }
        }
    }
    return result;
}

const matrixA = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
];

const matrixB = [
    [9, 8, 7],
    [6, 5, 4],
    [3, 2, 1]
];

console.log(multiplyMatrices(matrixA, matrixB));
```

Этот алгоритм умножения матриц имеет три вложенных цикла: первый проходится по
строкам первой матрицы, второй по столбцам второй матрицы, а третий - по общим
элементам для умножения. Количество операций равно n * n * n, где n - это размер
матрицы. Это приводит к кубической сложности O(n^3). Такой подход может стать
неэффективным для больших матриц из-за большого количества операций, которые
необходимы.

### Экспоненциальная сложность O(2^n)

Сложность O(2^n) относится к экспоненциальной сложности, где время выполнения
алгоритма увеличивается экспоненциально по мере увеличения размера входных
данных. Это часто встречается в алгоритмах, которые решают проблемы методом "
разделяй и властвуй" или используют рекурсию без оптимизации.

Примером алгоритма с экспоненциальной сложностью может служить рекурсивное
вычисление чисел Фибоначчи:

```javascript
function fibonacci(n) {
    if (n <= 1) {
        return n;
    } else {
        return fibonacci(n - 1) + fibonacci(n - 2);
    }
}

console.log(fibonacci(5)); // Результат: 5
console.log(fibonacci(10)); // Результат: 55
console.log(fibonacci(20)); // Результат: 6765
```

Этот код использует рекурсию для вычисления чисел Фибоначчи. Однако каждый раз,
когда вызывается функция fibonacci, она порождает два дополнительных вызова, что
приводит к экспоненциальному увеличению количества вызовов функций с увеличением
n.

Для больших значений n такой подход становится очень неэффективным из-за
огромного числа повторных вычислений. Экспоненциальная сложность обычно не
является оптимальным решением из-за своей высокой вычислительной нагрузки при
увеличении размера входных данных.

### Факториальная сложность O(n!)

Сложность O(n!) относится к факториальной сложности, где время выполнения
алгоритма растет пропорционально факториалу размера входных данных. Факториал -
это произведение всех положительных целых чисел от 1 до n.

Пример алгоритма с факториальной сложностью может быть перебор всех перестановок
элементов массива:

```javascript
function permute(arr) {
    function swap(a, b) {
        const temp = arr[a];
        arr[a] = arr[b];
        arr[b] = temp;
    }

    function generate(n) {
        if (n === 1) {
            console.log(arr);
        } else {
            for (let i = 0; i < n - 1; i++) {
                generate(n - 1);
                if (n % 2 === 0) {
                    swap(i, n - 1);
                } else {
                    swap(0, n - 1);
                }
            }
            generate(n - 1);
        }
    }

    generate(arr.length);
}

const myArray = [1, 2, 3];
permute(myArray);

```

Этот код создает все возможные перестановки элементов массива путем рекурсивного
генерирования всех возможных комбинаций. Количество операций, необходимых для
генерации всех перестановок, равно факториалу длины массива. Например, для
массива из 3 элементов (как в примере выше) будет 3! = 3 * 2 * 1 = 6
перестановок.

Такой алгоритм обладает очень высокой вычислительной сложностью и может стать
практически неиспользуемым для больших входных данных из-за огромного числа
операций, которые необходимо выполнить.

## График Big O

![big0.jpg](/img/algos/big0.jpg)

## Обзор алгоритмов по их сложности

![algoslist-bigo.jpg](/img/algos/algoslist-bigo.jpg)

### Сложность O(1)

Получение элементов массива по индексу

### Сложность O(n)

Линейный поиск.  
Чем больше элементов тем выше сложность.
Сумма элементов массива.

```javascript
const nums = [1, 2, 3, 4, 5];
let sum = 0;
for (let num of nums) {
    sum += num;
}
```

### Сложность O(n^2)

Алгоритм поиска дублей в массиве:

```javascript
const hasDuplicates = function (num) {
    //loop the list, our O(n) op
    for (let i = 0; i < nums.length; i++) {
        const thisNum = nums[i];
        //loop the list again, the O(n^2) op
        for (let j = 0; j < nums.length; j++) {
            //make sure we're not checking same number
            if (j !== i) {
                const otherNum = nums[j];
                //if there's an equal value, return
                if (otherNum === thisNum) return true;
            }
        }
    }
    //if we're here, no dups
    return false;
}
const nums = [1, 2, 3, 4, 5, 5];
hasDuplicates(nums);//true
```

### «Сложность порядка log n»: O(log n)

Быстрый обзор логарифмов

Рассмотрим пример, чему будет равен x?

x^3 = 8

Нужно взять кубический корень от 8 — это будет 2. Теперь посложнее

2^x = 512

С использованием логарифма задачу можно записать так

log2(512) = x

«логарифм по основанию 2 от 512 равен x». Обратите внимание «основание 2», т.е.
мы мыслим двойками — сколько раз нужно перемножить 2 что бы получить 512.

В алгоритме «бинарный поиск» на каждом шаге мы делим массив на две части.

Мое дополнение. Т.е. в худшем случае делаем столько операций, сколько раз можем
разделить массив на две части. Например, сколько раз мы можем разделить на две
части массив из 4 элементов? 2 раза. А массив из 8 элементов? 3 раза. Т.е.
кол-во делений/операций = log2(n) (где n кол-во элементов массива).

Получается, что зависимость кол-ва операций от кол-ва элементов ввода
описывается как log2(n)

## Мышление в терминах Big O

Получение элемента коллекции это O(1). Будь то получение по индексу в массиве,
или по ключу в словаре в нотации Big O это будет O(1)
Перебор коллекции это O(n)
Вложенные циклы по той же коллекции это O(n^2)
Разделяй и властвуй (Divide and Conquer) всегда O(log n)
Итерации которые используют Divide and Conquer это O(n log n)

## Как посчитать O(log2 n)

Чтобы посчитать логарифм `log₂(n)`, зная количество элементов `n`, ты можешь
использовать:

---

## ✅ 1. **Калькулятором**

На обычном калькуляторе чаще всего есть только `log₁₀(n)` (десятичный логарифм)
или `ln(n)` (натуральный). Чтобы получить `log₂(n)`:

### Формула:

\[
\log_2(n) = \frac{\log_{10}(n)}{\log_{10}(2)} \approx \frac{\log_{10}(n)
}{0.3010}
\]

---

## ✅ 2. **Примеры**

| n         | log₂(n) примерно |
|-----------|------------------|
| 1         | 0                |
| 2         | 1                |
| 4         | 2                |
| 8         | 3                |
| 16        | 4                |
| 32        | 5                |
| 100       | ≈ 6.64           |
| 1 000     | ≈ 9.97           |
| 1 000 000 | ≈ 19.93          |

---

## ✅ 3. **В Java (или любом языке)**

```java
int n = 1000;
double log2n = Math.log(n) / Math.log(2);
System.out.

println("log2("+n +") = "+log2n);
```

---

## 📌 Где применяется

- **В деревьях**: если у тебя 1000 элементов, и структура сбалансирована, то
  высота дерева = `log₂(1000) ≈ 10`, то есть максимум 10 шагов от корня до
  нужного элемента.
- **В бинарном поиске**: максимальное количество сравнений при поиске среди `n`
  элементов = `log₂(n)`

---

# Алгоритмы

# ⚙️ **Алгоритмы в Java**

## 1. **Сортировки**

- `Arrays.sort()` — быстрая сортировка, адаптированная Timsort.
- `Collections.sort()` — сортировка списков.

## 2. **Поиск**

- Линейный поиск — `O(n)`
- Бинарный поиск — `O(log n)`, работает только на отсортированных структурах.  
  Используется через: `Arrays.binarySearch()` или `Collections.binarySearch()`

## 3. **Хэширование**

- Используется в `HashMap`, `HashSet`.
- Ключевое понятие — метод `hashCode()` и `equals()`.

## 4. **Жадные алгоритмы, динамическое программирование, графы**

- Реализуются вручную.
- Можно использовать библиотеки (например, Apache Commons, Google Guava) для
  вспомогательных структур.

---

## 💡 Полезные классы и интерфейсы из Java Collections Framework

- `Iterable` — позволяет использовать цикл `for-each`
- `Comparator` / `Comparable` — для сортировки
- `PriorityQueue` — для алгоритмов типа Дейкстры
- `Stack` (устаревший, лучше `Deque`)
- `HashMap`, `TreeMap`, `LinkedHashMap`
- `ArrayDeque` — быстрая реализация стека/очереди

---

## 🚀 Советы:

- Для **интервью** полезно уметь реализовывать списки, деревья, сортировки
  вручную.
- Понимание **временной сложности (Big O)** критично.
- Изучи `Collections` и `Streams` для эффективной работы с данными.

############### Grok: Основные алгоритмы

В Java основные алгоритмы можно разделить на несколько категорий, включая
сортировку, поиск, работу с графами, обработку строк и другие. Вот краткий обзор
наиболее популярных и фундаментальных алгоритмов, которые часто реализуются на
Java:

### 1. **Алгоритмы сортировки**

- **Пузырьковая сортировка (Bubble Sort)**: Простой алгоритм, сравнивающий
  соседние элементы и меняющий их местами, если они в неправильном порядке.
  Сложность: O(n²).
- **Сортировка выбором (Selection Sort)**: Находит минимальный элемент и
  помещает его в начало. Сложность: O(n²).
- **Сортировка вставками (Insertion Sort)**: Постепенно строит отсортированный
  массив, вставляя элементы в правильные позиции. Сложность: O(n²).
- **Быстрая сортировка (Quick Sort)**: Использует стратегию "разделяй и
  властвуй", выбирая опорный элемент. Средняя сложность: O(n log n).
- **Сортировка слиянием (Merge Sort)**: Разделяет массив на части, сортирует их
  и объединяет. Сложность: O(n log n).
- **Сортировка кучей (Heap Sort)**: Использует структуру кучи для сортировки.
  Сложность: O(n log n).
- **Arrays.sort()**: Встроенная в Java реализация (Dual-Pivot QuickSort для
  примитивов и TimSort для объектов).

### 2. **Алгоритмы поиска**

- **Линейный поиск (Linear Search)**: Проверяет каждый элемент массива.
  Сложность: O(n).
- **Бинарный поиск (Binary Search)**: Работает на отсортированном массиве, деля
  его пополам. Сложность: O(log n). Реализация: `Arrays.binarySearch()`.
- **Поиск в глубину (Depth-First Search, DFS)**: Используется для графов и
  деревьев.
- **Поиск в ширину (Breadth-First Search, BFS)**: Исследует узлы графа по
  уровням.

### 3. **Алгоритмы для графов**

- **Алгоритм Дейкстры (Dijkstra’s Algorithm)**: Находит кратчайший путь в графе
  с неотрицательными весами.
- **Алгоритм Краскала (Kruskal’s Algorithm)**: Находит минимальное остовное
  дерево в графе.
- **Алгоритм Прима (Prim’s Algorithm)**: Альтернатива для поиска минимального
  остовного дерева.
- **Алгоритм Флойда-Уоршалла (Floyd-Warshall)**: Находит кратчайшие пути между
  всеми парами вершин.
- **Топологическая сортировка**: Применяется для направленных ациклических
  графов (DAG).

### 4. **Алгоритмы для строк**

- **Алгоритм Кнута-Морриса-Пратта (KMP)**: Эффективный поиск подстроки в строке.
  Сложность: O(n + m).
- **Алгоритм Бойера-Мура (Boyer-Moore)**: Оптимизированный поиск подстроки.
- **Поиск по шаблону с регулярными выражениями**: Используется в Java через
  класс `Pattern` и `Matcher`.

### 5. **Алгоритмы для работы с деревьями**

- **Обход дерева**: Прямой (pre-order), симметричный (in-order), обратный (
  post-order).
- **Бинарное дерево поиска (Binary Search Tree, BST)**: Операции вставки,
  удаления, поиска.
- **Сбалансированные деревья**: AVL-дерево, красно-черное дерево (используется в
  `TreeMap` и `TreeSet`).

### 6. **Динамическое программирование**

- **Задача о рюкзаке (Knapsack Problem)**: Оптимизация выбора предметов с
  ограничением по весу.
- **Последовательность Фибоначчи**: Простой пример с мемоизацией.
- **Наибольшая общая подпоследовательность (Longest Common Subsequence, LCS)**:
  Для сравнения строк.

### 7. **Жадные алгоритмы**

- **Алгоритм Хаффмана (Huffman Coding)**: Для сжатия данных.
- **Задача о выборе активностей (Activity Selection)**: Выбор максимального
  числа непересекающихся задач.

### 8. **Другие полезные алгоритмы**

- **Алгоритм Евклида**: Для нахождения НОД двух чисел.
- **Быстрое возведение в степень**: Оптимизация вычислений степеней.
- **Решето Эратосфена**: Для нахождения простых чисел.

### Реализация в Java

Java предоставляет встроенные классы и методы для многих алгоритмов:

- **Коллекции (`java.util`)**: `ArrayList`, `HashMap`, `TreeSet`,
  `PriorityQueue` упрощают реализацию.
- **Arrays и Collections**: Методы для сортировки и поиска.
- **Stream API**: Для функциональной обработки данных.

Если вам нужен пример реализации конкретного алгоритма на Java, напишите, и я
предоставлю код!

## Алгоритмы работы с коллекциями

В Java для работы с коллекциями (из пакета `java.util`) используются различные
алгоритмы, предоставленные в основном классом `Collections` и другими утилитами.
Вот основные алгоритмы и методы для работы с коллекциями:

### 1. **Сортировка**

- **`Collections.sort(List<T> list)`**: Сортирует список в естественном
  порядке (для элементов, реализующих интерфейс `Comparable`).
- **`Collections.sort(List<T> list, Comparator<? super T> c)`**: Сортирует
  список с использованием пользовательского компаратора.
- **Пример**:
  ```java
  List<Integer> list = Arrays.asList(5, 2, 8, 1);
  Collections.sort(list); // [1, 2, 5, 8]
  ```

### 2. **Поиск**

- **`Collections.binarySearch(List<? extends Comparable<? super 
T>> list, T key)`**: Выполняет бинарный поиск в отсортированном списке.
  Возвращает индекс
  элемента или отрицательное значение, если элемент не найден.

- **
  `Collections.binarySearch(List<? extends T> list, T key, Comparator<? super T> c)`
  **: Бинарный поиск с использованием компаратора.
- **Примечание**: Список должен быть предварительно отсортирован.
- **Пример**:
  ```java
  List<Integer> list = Arrays.asList(1, 2, 5, 8);
  int index = Collections.binarySearch(list, 5); // 2
  ```

### 3. **Перемешивание**

- **`Collections.shuffle(List<?> list)`**: Случайно перемешивает элементы в
  списке.
- **`Collections.shuffle(List<?> list, Random rnd)`**: Перемешивание с заданным
  генератором случайных чисел.
- **Пример**:
  ```java
  List<Integer> list = Arrays.asList(1, 2, 3, 4);
  Collections.shuffle(list); // Например, [3, 1, 4, 2]
  ```

### 4. **Обратный порядок**

- **`Collections.reverse(List<?> list)`**: Изменяет порядок элементов в списке
  на обратный.
- **Пример**:
  ```java
  List<Integer> list = Arrays.asList(1, 2, 3);
  Collections.reverse(list); // [3, 2, 1]
  ```

### 5. **Заполнение**

- **`Collections.fill(List<? super T> list, T obj)`**: Заменяет все элементы
  списка указанным объектом.
- **Пример**:
  ```java
  List<String> list = new ArrayList<>(Arrays.asList("a", "b", "c"));
  Collections.fill(list, "x"); // [x, x, x]
  ```

### 6. **Копирование**

- **`Collections.copy(List<? super T> dest, List<? extends T> src)`**: Копирует
  элементы из исходного списка в целевой. Целевой список должен быть не короче
  исходного.
- **Пример**:
  ```java
  List<Integer> src = Arrays.asList(1, 2, 3);
  List<Integer> dest = new ArrayList<>(Arrays.asList(0, 0, 0, 0));
  Collections.copy(dest, src); // dest: [1, 2, 3, 0]
  ```

### 7. **Минимальный и максимальный элементы**

- **`Collections.min(Collection<? extends T> coll)`**: Находит минимальный
  элемент в коллекции (для элементов, реализующих `Comparable`).
- **`Collections.max(Collection<? extends T> coll)`**: Находит максимальный
  элемент.
- **
  `Collections.min/max(Collection<? extends T> coll, Comparator<? super T> comp)`
  **: С использованием компаратора.
- **Пример**:
  ```java
  List<Integer> list = Arrays.asList(5, 2, 8, 1);
  int min = Collections.min(list); // 1
  int max = Collections.max(list); // 8
  ```

### 8. **Частота и подсчет**

- **`Collections.frequency(Collection<?> c, Object o)`**: Подсчитывает
  количество вхождений элемента в коллекции.
- **Пример**:
  ```java
  List<String> list = Arrays.asList("a", "b", "a", "c");
  int freq = Collections.frequency(list, "a"); // 2
  ```

### 9. **Проверка непересечения**

- **`Collections.disjoint(Collection<?> c1, Collection<?> c2)`**: Проверяет, нет
  ли общих элементов в двух коллекциях.
- **Пример**:
  ```java
  List<Integer> list1 = Arrays.asList(1, 2, 3);
  List<Integer> list2 = Arrays.asList(4, 5, 6);
  boolean disjoint = Collections.disjoint(list1, list2); // true
  ```

### 10. **Синхронизация**

- **`Collections.synchronizedCollection(Collection<T> c)`**: Создает
  синхронизированную (потокобезопасную) версию коллекции.
- **`Collections.synchronizedList(List<T> c)`**, *
  *`Collections.synchronizedSet(Set<T> c)`**, и т.д.
- **Пример**:
  ```java
  List<String> list = Collections.synchronizedList(new ArrayList<>());
  ```

### 11. **Неизменяемые коллекции**

- **`Collections.unmodifiableCollection(Collection<? extends T> c)`**: Создает
  неизменяемую обертку над коллекцией.
- Аналогичные методы: `unmodifiableList`, `unmodifiableSet`, `unmodifiableMap`,
  и т.д.
- **Пример**:
  ```java
  List<String> list = Collections.unmodifiableList(Arrays.asList("a", "b"));
  // list.add("c"); // UnsupportedOperationException
  ```

### 12. **Пакетные операции**

- **`List.replaceAll(UnaryOperator<E> operator)`**: Заменяет каждый элемент
  списка результатом применения функции.
- **`List.removeIf(Predicate<? super E> filter)`**: Удаляет элементы,
  удовлетворяющие условию.
- **Пример**:
  ```java
  List<Integer> list = new ArrayList<>(Arrays.asList(1, 2, 3, 4));
  list.replaceAll(x -> x * 2); // [2, 4, 6, 8]
  list.removeIf(x -> x > 5); // [2, 4]
  ```

### 13. **Stream API (Java 8+)**

Хотя это не часть `Collections`, Stream API предоставляет мощные функциональные
алгоритмы для работы с коллекциями:

- **Фильтрация**: `stream().filter(...)`
- **Сортировка**: `stream().sorted(...)`
- **Маппинг**: `stream().map(...)`
- **Сбор**: `stream().collect(...)`
- **Пример**:
  ```java
  List<Integer> list = Arrays.asList(1, 2, 3, 4);
  List<Integer> result = list.stream()
                             .filter(x -> x % 2 == 0)
                             .map(x -> x * 2)
                             .collect(Collectors.toList()); // [4, 8]
  ```

### 14. **Другие полезные методы**

- **`Collections.swap(List<?> list, int i, int j)`**: Меняет местами элементы по
  указанным индексам.
- **`Collections.rotate(List<?> list, int distance)`**: Циклически сдвигает
  элементы списка.
- **`Collections.nCopies(int n, T o)`**: Создает неизменяемый список из `n`
  копий элемента.
- **Пример**:
  ```java
  List<Integer> list = new ArrayList<>(Arrays.asList(1, 2, 3));
  Collections.swap(list, 0, 2); // [3, 2, 1]
  ```

### Примечания

- Большинство методов в `Collections` работают с интерфейсами (`List`, `Set`,
  `Collection`), но некоторые (например, `sort`, `reverse`) требуют `List`.
- Для потокобезопасности используйте синхронизированные коллекции или классы из
  пакета `java.util.concurrent` (например, `CopyOnWriteArrayList`).
- Stream API часто предпочтительнее для сложных операций благодаря
  выразительности и возможности параллельного выполнения.

# Подробно

Алгоритмы сортироваки, алгоритмы поиска, алгоритмы обхода графов/деревьев,
алгоритмы работы со списками (односвязными/двусвязными и тп) и тд… В общем,
базовые вещи с первых курсов универа…

## Обзор основных типов алгоритмов

1. Алгоритмы сортировки:

Помогают нам упорядочить данные в нужном порядке.
Мы можем использовать их для сортировки чисел, слов, и других элементов.
Примеры — включают пузырьковую сортировку, сортировку вставками и быструю
сортировку.
Могут быть полезны при работе с большими объемами данных или при необходимости
быстро находить нужные элементы.

2. Алгоритмы поиска:

Помогают найти определенный элемент в заданном наборе данных.
Мы можем использовать их для поиска элемента в массиве, списке или базе данных.
Примеры включают линейный поиск, двоичный поиск и поиск с использованием
хэш-таблиц.
Также помогают нам эффективно находить нужную информацию и ускорять выполнение
программ.

Динамическое программирование:

Решает сложные задачи, разбивая их на более простые подзадачи.
Мы можем использовать его для оптимизации решений и улучшения
производительности.
Примеры включают задачу с рюкзаком, нахождение наибольшей общей
подпоследовательности и вычисление чисел Фибоначчи.
Также позволяет нам решать сложные задачи эффективно и снижать время выполнения
программ.

## Погружение в алгоритмы сортировки

### Пузырьковая сортировка:

Этот алгоритм проходит по списку данных несколько раз и меняет местами соседние
элементы, если они находятся в неправильном порядке.
Мы продолжаем проходить по списку до тех пор, пока все элементы не будут
упорядочены.
Например, если у нас есть список чисел [5, 2, 7, 1, 3], то пузырьковая
сортировка сделает следующие
шаги: [2, 5, 7, 1, 3] -> [2, 5, 1, 7, 3] -> [2, 5, 1, 3, 7] -> [2, 1, 5, 3, 7] -> [2, 1, 3, 5, 7] -> [1, 2, 3, 5, 7].

### Сортировка вставками:

Этот алгоритм строит отсортированную последовательность элементов, один за
другим, путем вставки каждого элемента на правильное место.
Мы начинаем с пустого списка и последовательно добавляем элементы в правильные
позиции.
Например, если у нас есть список чисел [5, 2, 7, 1, 3], то сортировка вставками
сделает следующие
шаги: [5] -> [2, 5] -> [2, 5, 7] -> [1, 2, 5, 7] -> [1, 2, 3, 5, 7].

### Быстрая сортировка:

Этот алгоритм использует стратегию «разделяй и властвуй». Мы выбираем опорный
элемент из списка, разделяем остальные элементы на две части: одна с элементами
меньше опорного, другая — с элементами больше опорного.
Затем мы рекурсивно применяем быструю сортировку к обеим частям списка.
Например, если у нас есть список чисел [5, 2, 7, 1, 3], то быстрая сортировка
может сделать следующие
шаги: [3, 2, 1] -> [1, 2, 3] -> [5, 7] -> [1, 2, 3, 5, 7].
Каждый из примеров имеет свои преимущества и подходит для разных ситуаций.

## Погружение в алгоритмы поиска

Когда у нас есть огромный набор данных и нам нужно найти конкретный элемент,
алгоритмы поиска приходят на помощь. Давай рассмотрим несколько примеров и
объясним, как работают эти алгоритмы.

### Линейный поиск:

Это самый простой алгоритм поиска. Мы просто проходим по списку элементов
последовательно, сравнивая каждый элемент с искомым, пока не найдем совпадение
или не пройдем весь список.
Например, если у нас есть список чисел [5, 2, 7, 1, 3] и мы ищем число 7, то
линейный поиск будет последовательно сравнивать каждый элемент: 5 != 7, 2 != 7,
7 == 7 — совпадение найдено!

### Бинарный поиск:

Этот алгоритм работает только для упорядоченных списков. Мы сравниваем искомый
элемент с элементом в середине списка. Если они совпадают, то поиск завершается.
Если искомый элемент больше, мы исключаем левую половину списка и повторяем
поиск в правой половине. Если искомый элемент меньше, то наоборот — исключаем
правую половину и продолжаем поиск в левой.
Например, если у нас есть упорядоченный список чисел [1, 2, 3, 5, 7] и мы ищем
число 7, то бинарный поиск сначала сравнивает его с элементом в середине (3).
Так как искомое число больше, мы исключаем левую половину списка [1, 2, 3] и
продолжаем поиск в правой половине [5, 7]. Затем мы снова сравниваем его с
элементом в середине (7) и находим совпадение!

### Алгоритмы поиска в графах:

Поиск в графах — это еще более сложная тема, но вкратце, эти алгоритмы помогают
находить определенные элементы или пути в графе, которые состоят из вершин и
ребер (соединений между вершинами).
Один из популярных алгоритмов — это поиск в ширину (BFS), который ищет ближайший
путь от одной вершины к другой, и поиск в глубину (DFS), который ищет все
возможные пути в графе.
Например, если у нас есть графа с вершинами и ребрами, и мы хотим найти
кратчайший путь от вершины A к вершине B, алгоритм BFS будет последовательно
исследовать все вершины, пока не найдет путь от A до B.

## Погружение в динамическое программирование

Это подход к решению сложных задач, который основан на разбиении их на более
простые подзадачи и сохранении результатов этих подзадач для более эффективного
решения.

### Нахождение чисел Фибоначчи:

Каждое число Фибоначчи равно сумме двух предыдущих чисел: 0, 1, 1, 2, 3, 5, 8 и
так далее.
Вместо того чтобы вычислять каждое число Фибоначчи отдельно, мы можем
использовать динамическое программирование, чтобы сохранить результаты уже
вычисленных чисел и использовать их для вычисления следующих. Таким образом, мы
избегаем повторных вычислений и значительно ускоряем процесс.

### Задача с рюкзаком:

Представьте, что у вас есть рюкзак с ограниченной вместимостью, и у вас есть
набор предметов с определенными весами и стоимостями.
Задача состоит в том, чтобы выбрать такой набор предметов, чтобы их суммарная
стоимость была максимальной, при условии, что их суммарный вес не превышает
вместимость рюкзака. Динамическое программирование позволяет нам эффективно
решать эту задачу, сохраняя результаты для каждого подмножества предметов и
используя их для построения оптимального решения.

### Расстояние Левенштейна:

Это метрика, используемая для измерения разницы между двумя строками.
Определяется как минимальное количество операций вставки, удаления и замены
символов, необходимых для превращения одной строки в другую.
Мы можем построить матрицу, где каждый элемент представляет собой расстояние
между подстроками исходных строк, и использовать сохраненные результаты для
вычисления расстояний для более длинных подстрок.

