# Теория по алгоритмам

## Оценка сложности алгоритмов Big O

Сложность алгоритмов оценивают по времени выполнения задачи или по используемой
памяти.
Сложность зависит от размеров входных данных: массив из 100 элементов будет
обработан быстрее, чем аналогичный из 1000.

Big O показывает верхнюю границу (худший вариант) того, как сложность алгоритма
растёт с увеличением входных данных.

**Сложность алгоритма может быть:**

**О(1)** – константная сложность. Время выполнения алгоритма не зависит от
размера входных данных. Например: получить элемент массива по индексу.

**O(log n)** – логарифмическая сложность<br>
Сложность алгоритма растёт логарифмически с увеличением входных данных.
Например, бинарный поиск в отсортированном массиве.

**О(n)** – линейная сложность. Сложность алгоритма растёт линейно с увеличением
входных данных. Например: цикл по всем элементам массива, рекурсивная функция.

**O(n log n)** - Линейно-логарифмическая сложность. Время выполнения алгоритма
растет быстрее, чем линейно, но медленнее, чем квадратично. Например, сортировка
слиянием (merge sort).

**O(n^2)** - Квадратичная сложность. Время выполнения алгоритма зависит от
квадрата размера входных данных. Например, сортировка пузырьком (bubble sort).

**O(n^3)** - Кубическая сложность. Время выполнения алгоритма зависит от размера
входных данных в кубе. Например, алгоритмы, которые имеют три вложенных цикла,
такие как некоторые методы многомерной обработки данных.

**O(n!)** - Факториальная сложность. Это самая высокая степень роста времени
выполнения алгоритма. Время выполнения алгоритма растет факториально от размера
входных данных. Этот тип сложности встречается, например, при переборе всех
возможных комбинаций элементов, что делает его чрезвычайно неэффективным для
больших значений n.

**Другие обозначения сложности алгоритмов.**<br>
Кроме обозначения "Big O", существуют другие обозначения для оценки сложности
алгоритмов.

- Big Theta (Θ): Big Theta также оценивает верхнюю и нижнюю границы временной
  сложности алгоритма, но описывает точную сложность, а не только наихудший
  случай, как Big O. Θ(f(n)) обозначает, что время выполнения алгоритма
  ограничено функцией f(n) как сверху, так и снизу.
- Big Omega (Ω): Big Omega оценивает нижнюю границу временной сложности
  алгоритма. Ω(f(n)) говорит о том, что алгоритм выполнится не быстрее, чем
  функция f(n).
- Little O (o): Little O представляет собой верхнюю границу, которая строже, чем
  Big O. Если f(n) является o(g(n)), это означает, что время выполнения
  алгоритма ограничивается функцией g(n), но алгоритм работает быстрее, чем g(
  n).
- Little Omega (ω): Little Omega представляет собой нижнюю границу, которая
  строже, чем Big Omega. Если f(n) является ω(g(n)), это означает, что алгоритм
  работает медленнее, чем g(n), но не медленнее, чем f(n).

### Константная сложность O(1)

O(1) называют константной сложностью.

Оценка временной сложности O(1) означает, что алгоритм имеет постоянную
сложность.

Под n мы будем иметь ввиду размер входящих данных.

В JavaScript чаще всего мы храним набор данных в массивах.

Размер массива, то есть его значение свойства length и будет значение n.

При константной сложности вне зависимости от размера входных данных (n), время
выполнения алгоритма остается постоянным и не зависит от объема данных.

Это самый быстрый и эффективный вид временной сложности.

Примеры алгоритмов с оценкой временной сложности O(1):

- Доступ к элементу в массиве по индексу. Например, если у вас есть массив с
  данными, вы можете мгновенно получить доступ к элементу массива, указав его
  индекс, независимо от размера массива.

- Вставка или удаление элемента в конец списка (очереди) фиксированной длины. В
  этом случае операция выполняется быстро и не зависит от количества элементов в
  списке.

Оценка временной сложности O(1) является идеальной с точки зрения
производительности. Однако в реальных задачах она не всегда достижима, и в
большинстве случаев оценка временной сложности будет выше.

```javascript
    function getLastElement(arr) {
    return arr[arr.length - 1]
}

```

Функция getLastElement имеет временную сложность O(1) потому, что она выполняет
одну операцию - получение последнего элемента массива arr.

Независимо от размера массива, функция всегда выполняет одно действие, поэтому
ее сложность остается постоянной и равной O(1). Это означает, что время
выполнения функции не зависит от количества элементов в массиве и всегда будет
быстрым и постоянным.

### Линейная сложность O(n)

O(n) называют линейной сложностью.

Линейный рост - это понятие, описывающее зависимость между двумя
величинами, при которой одна величина увеличивается пропорционально увеличению
другой.

Оценка временной сложности O(n) означает, что время выполнения алгоритма растет
линейно с увеличением размера входных данных.

```javascript
    function findMax(arr) {
    let max = arr[0];
    for (let i = 1; i < arr.length; i++) {
        if (arr[i] > max) {
            max = arr[i];
        }
    }
    return max;
}

```

В этом примере findMax ищет максимальное значение в массиве arr.

Алгоритм начинает с предположения, что первый элемент массива (arr[0]) является
максимальным, а затем линейно (то есть по одному элементу) перебирает остальные
элементы, сравнивая каждый с текущим максимальным. Сложность этого алгоритма O(
n), так как время выполнения растет линейно с увеличением количества элементов в
массиве arr.

При линейном росте при увеличении размера входных данных вдвое, то время
выполнения алгоритма также увеличится примерно вдвое. Если увеличите размер
данных в 10 раз, то время выполнения увеличится приблизительно в 10 раз, и так
далее.

Линейный рост характерен для алгоритмов, которые выполняют постоянное количество
операций для каждого элемента входных данных.

### Логарифмическая сложность O(log n)

**Сложность алгоритма \( O(\log n) \)**, или **логарифмическая сложность**,
означает, что время выполнения алгоритма (или количество операций) растёт
пропорционально логарифму от размера входных данных \( n \). Это одна из самых
эффективных сложностей для больших данных, так как рост времени замедляется по
мере увеличения \( n \).

### Объяснение:

- Логарифмическая сложность обычно возникает в алгоритмах, которые на каждой
  итерации **делят задачу на части** (например, на половину), значительно
  сокращая объем работы.
- Логарифм (обычно основание 2, \( \log_2 n \)) показывает, сколько раз можно
  разделить \( n \) на 2, чтобы получить 1.
    - Например, если \( n = 8 \), то \( \log_2 8 = 3 \), так как \( 8 \div 2 =
      4 \), \( 4 \div 2 = 2 \), \( 2 \div 2 = 1 \) (3 шага).
    - Для \( n = 1024 \), \( \log_2 1024 = 10 \).

### Примеры алгоритмов с \( O(\log n) \):

1. **Бинарный поиск (Binary Search)**:

- На каждой итерации массив делится пополам, пока не найдется элемент.
- Пример: Поиск в отсортированном массиве.
- Сложность: \( O(\log n) \).

2. **Операции в сбалансированных бинарных деревьях поиска (BST)**:

- Вставка, поиск и удаление в AVL-дереве или красно-черном дереве (например, в
  `TreeMap` или `TreeSet` в Java).
- Высота дерева \( \approx \log n \), поэтому операции занимают \( O(\log n) \).

3. **Двоичный поиск в структурах данных**:

- Например, поиск в отсортированном наборе или определение границ в задачах.

### Почему \( O(\log n) \) эффективно?

- Для больших \( n \) логарифмическая сложность растёт очень медленно:
    - \( n = 10^3 \): \( \log_2 n \approx 10 \).
    - \( n = 10^6 \): \( \log_2 n \approx 20 \).
    - \( n = 10^9 \): \( \log_2 n \approx 30 \).
- Это делает алгоритмы с \( O(\log n) \) подходящими для обработки огромных
  объемов данных.

### Сравнение с другими сложностями:

- \( O(1) \): Постоянное время, не зависит от \( n \).
- \( O(\log n) \): Логарифмическое время, очень эффективно.
- \( O(n) \): Линейное время, пропорционально \( n \).
- \( O(n \log n) \): Линейно-логарифмическое, типично для сортировок (например,
  `Arrays.sort` в Java).
- \( O(n^2) \): Квадратичное время, менее эффективно для больших \( n \).

### Пример в Java (бинарный поиск):

```java
public int binarySearch(int[] arr, int target) {
    int left = 0;
    int right = arr.length - 1;

    while (left <= right) {
        int mid = left + (right - left) / 2; // Находим середину
        if (arr[mid] == target) {
            return mid;
        } else if (arr[mid] < target) {
            left = mid + 1; // Исключаем левую половину
        } else {
            right = mid - 1; // Исключаем правую половину
        }
    }
    return -1; // Элемент не найден
}
```

- На каждой итерации размер рассматриваемого массива уменьшается вдвое, поэтому
  сложность \( O(\log n) \).

### Связь с бинарным деревом поиска:

В сбалансированном бинарном дереве поиска (BST) высота дерева составляет \( O(
\log n) \), так как каждый уровень делит узлы примерно пополам. Это обеспечивает
логарифмическую сложность для операций поиска, вставки и удаления. Однако в
несбалансированном BST (например, вырожденном в список) сложность может
ухудшиться до \( O(n) \).

### Заключение:

\( O(\log n) \) описывает алгоритмы, которые эффективно сокращают объем работы
на каждой итерации, разделяя данные на части (обычно пополам). Это характерно
для бинарного поиска, операций в сбалансированных деревьях и других алгоритмов,
работающих с иерархическими структурами. Если нужен пример конкретного алгоритма
или дополнительное объяснение, дайте знать!

### Квадратичная сложность O(n^2)

O(n^2) означает квадратичную сложность алгоритма, где время выполнения растет
пропорционально квадрату размера входных данных. Это часто возникает в
алгоритмах с вложенными циклами, когда каждый элемент первого списка
обрабатывается с каждым элементом второго списка.

Вот пример простого алгоритма с квадратичной сложностью, который ищет сумму всех
пар элементов в массиве:

```javascript
function sumOfPairs(arr) {
    let sum = 0;
    for (let i = 0; i < arr.length; i++) {
        for (let j = 0; j < arr.length; j++) {
            sum += arr[i] + arr[j];
        }
    }
    return sum;
}

const myArray = [1, 2, 3, 4];
console.log(sumOfPairs(myArray)); // Результат: 40
```

Этот код имеет два вложенных цикла, каждый из которых проходится по всему
массиву. Количество операций в циклах равно n * n, где n - это длина массива.
Это приводит к квадратичной сложности O(n^2), что может сделать его
неэффективным для больших массивов из-за большого количества операций,
выполняемых на каждый элемент.

### Кубическая сложность O(n^3)

Cложность O(n^3) означает, что время выполнения алгоритма увеличивается
кубически по размеру входных данных. Это часто встречается в алгоритмах, где
есть три вложенных цикла или операции, каждая из которых выполняется
пропорционально кубу размера входных данных.

Вот пример алгоритма с кубической сложностью, который умножает две матрицы:

```javascript
function multiplyMatrices(matrix1, matrix2) {
    let result = [];
    const m = matrix1.length;
    const n = matrix2[0].length;
    const p = matrix2.length;

    for (let i = 0; i < m; i++) {
        result[i] = [];
        for (let j = 0; j < n; j++) {
            result[i][j] = 0;
            for (let k = 0; k < p; k++) {
                result[i][j] += matrix1[i][k] * matrix2[k][j];
            }
        }
    }
    return result;
}

const matrixA = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
];

const matrixB = [
    [9, 8, 7],
    [6, 5, 4],
    [3, 2, 1]
];

console.log(multiplyMatrices(matrixA, matrixB));
```

Этот алгоритм умножения матриц имеет три вложенных цикла: первый проходится по
строкам первой матрицы, второй по столбцам второй матрицы, а третий - по общим
элементам для умножения. Количество операций равно n * n * n, где n - это размер
матрицы. Это приводит к кубической сложности O(n^3). Такой подход может стать
неэффективным для больших матриц из-за большого количества операций, которые
необходимы.

### Экспоненциальная сложность O(2^n)

Сложность O(2^n) относится к экспоненциальной сложности, где время выполнения
алгоритма увеличивается экспоненциально по мере увеличения размера входных
данных. Это часто встречается в алгоритмах, которые решают проблемы методом "
разделяй и властвуй" или используют рекурсию без оптимизации.

Примером алгоритма с экспоненциальной сложностью может служить рекурсивное
вычисление чисел Фибоначчи:

```javascript
function fibonacci(n) {
    if (n <= 1) {
        return n;
    } else {
        return fibonacci(n - 1) + fibonacci(n - 2);
    }
}

console.log(fibonacci(5)); // Результат: 5
console.log(fibonacci(10)); // Результат: 55
console.log(fibonacci(20)); // Результат: 6765
```

Этот код использует рекурсию для вычисления чисел Фибоначчи. Однако каждый раз,
когда вызывается функция fibonacci, она порождает два дополнительных вызова, что
приводит к экспоненциальному увеличению количества вызовов функций с увеличением
n.

Для больших значений n такой подход становится очень неэффективным из-за
огромного числа повторных вычислений. Экспоненциальная сложность обычно не
является оптимальным решением из-за своей высокой вычислительной нагрузки при
увеличении размера входных данных.

### Факториальная сложность O(n!)

Сложность O(n!) относится к факториальной сложности, где время выполнения
алгоритма растет пропорционально факториалу размера входных данных. Факториал -
это произведение всех положительных целых чисел от 1 до n.

Пример алгоритма с факториальной сложностью может быть перебор всех перестановок
элементов массива:

```javascript
function permute(arr) {
    function swap(a, b) {
        const temp = arr[a];
        arr[a] = arr[b];
        arr[b] = temp;
    }

    function generate(n) {
        if (n === 1) {
            console.log(arr);
        } else {
            for (let i = 0; i < n - 1; i++) {
                generate(n - 1);
                if (n % 2 === 0) {
                    swap(i, n - 1);
                } else {
                    swap(0, n - 1);
                }
            }
            generate(n - 1);
        }
    }

    generate(arr.length);
}

const myArray = [1, 2, 3];
permute(myArray);

```

Этот код создает все возможные перестановки элементов массива путем рекурсивного
генерирования всех возможных комбинаций. Количество операций, необходимых для
генерации всех перестановок, равно факториалу длины массива. Например, для
массива из 3 элементов (как в примере выше) будет 3! = 3 * 2 * 1 = 6
перестановок.

Такой алгоритм обладает очень высокой вычислительной сложностью и может стать
практически неиспользуемым для больших входных данных из-за огромного числа
операций, которые необходимо выполнить.

## График Big O

![big0.jpg](/img/algos/big0.jpg)

## Обзор алгоритмов по их сложности

![algoslist-bigo.jpg](/img/algos/algoslist-bigo.jpg)

### Сложность O(1)

Получение элементов массива по индексу

### Сложность O(n)

Линейный поиск.  
Чем больше элементов тем выше сложность.
Сумма элементов массива.

```javascript
const nums = [1, 2, 3, 4, 5];
let sum = 0;
for (let num of nums) {
    sum += num;
}
```

### Сложность O(n^2)

Алгоритм поиска дублей в массиве:

```javascript
const hasDuplicates = function (num) {
    //loop the list, our O(n) op
    for (let i = 0; i < nums.length; i++) {
        const thisNum = nums[i];
        //loop the list again, the O(n^2) op
        for (let j = 0; j < nums.length; j++) {
            //make sure we're not checking same number
            if (j !== i) {
                const otherNum = nums[j];
                //if there's an equal value, return
                if (otherNum === thisNum) return true;
            }
        }
    }
    //if we're here, no dups
    return false;
}
const nums = [1, 2, 3, 4, 5, 5];
hasDuplicates(nums);//true
```

### «Сложность порядка log n»: O(log n)

Быстрый обзор логарифмов

Рассмотрим пример, чему будет равен x?

x^3 = 8

Нужно взять кубический корень от 8 — это будет 2. Теперь посложнее

2^x = 512

С использованием логарифма задачу можно записать так

log2(512) = x

«логарифм по основанию 2 от 512 равен x». Обратите внимание «основание 2», т.е.
мы мыслим двойками — сколько раз нужно перемножить 2 что бы получить 512.

В алгоритме «бинарный поиск» на каждом шаге мы делим массив на две части.

Мое дополнение. Т.е. в худшем случае делаем столько операций, сколько раз можем
разделить массив на две части. Например, сколько раз мы можем разделить на две
части массив из 4 элементов? 2 раза. А массив из 8 элементов? 3 раза. Т.е.
кол-во делений/операций = log2(n) (где n кол-во элементов массива).

Получается, что зависимость кол-ва операций от кол-ва элементов ввода
описывается как log2(n)

## Мышление в терминах Big O

Получение элемента коллекции это O(1). Будь то получение по индексу в массиве,
или по ключу в словаре в нотации Big O это будет O(1)
Перебор коллекции это O(n)
Вложенные циклы по той же коллекции это O(n^2)
Разделяй и властвуй (Divide and Conquer) всегда O(log n)
Итерации которые используют Divide and Conquer это O(n log n)

## Как посчитать O(log2 n)

Чтобы посчитать логарифм `log₂(n)`, зная количество элементов `n`, ты можешь
использовать:

---

## ✅ 1. **Калькулятором**

На обычном калькуляторе чаще всего есть только `log₁₀(n)` (десятичный логарифм)
или `ln(n)` (натуральный). Чтобы получить `log₂(n)`:

### Формула:

\[
\log_2(n) = \frac{\log_{10}(n)}{\log_{10}(2)} \approx \frac{\log_{10}(n)
}{0.3010}
\]

---

## ✅ 2. **Примеры**

| n         | log₂(n) примерно |
|-----------|------------------|
| 1         | 0                |
| 2         | 1                |
| 4         | 2                |
| 8         | 3                |
| 16        | 4                |
| 32        | 5                |
| 100       | ≈ 6.64           |
| 1 000     | ≈ 9.97           |
| 1 000 000 | ≈ 19.93          |

---

## ✅ 3. **В Java (или любом языке)**

```java
int n = 1000;
double log2n = Math.log(n) / Math.log(2);
System.out.

println("log2("+n +") = "+log2n);
```

---

## 📌 Где применяется

- **В деревьях**: если у тебя 1000 элементов, и структура сбалансирована, то
  высота дерева = `log₂(1000) ≈ 10`, то есть максимум 10 шагов от корня до
  нужного элемента.
- **В бинарном поиске**: максимальное количество сравнений при поиске среди `n`
  элементов = `log₂(n)`

---

Хочешь, я сделаю для тебя таблицу `n` и `log₂(n)` для часто встречающихся
размеров?